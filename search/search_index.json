{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DYNAMO QuickStart User Guide","text":""},{"location":"#overview","title":"Overview","text":"<p>DYNAMO assists an analyst to easily use sophisticated simulation models and data in order to explore the role of weather and climate in water on food availability in select regions of the world. For example, an analyst can use DYNAMO to investigate the expected crop yields given different rainfall predictions through its effect on flooding and drought. DYNAMO\u2019s simulation models are quantitative and contain extensive subject matter knowledge. For example, a hydrology model contains physical laws that describe how water moves through a river basin, and uses data about the elevation of the terrain and the soil types to determine how much water is absorbed in the ground and how the water flows over a land surface.</p> <p>Different analysts may have different expertise and run different types of models. Each analyst is given a separate account in DYNAMO, and their activities noted with their user name. All analysts can see the same information in their interface, so when one completes a task all the results are accessible to all the analysts. An analyst can also use DYNAMO to investigate possible interventions. For example, changing planting windows to an earlier time might increase crop production, which can be analyzed using an agriculture model. Another possible intervention to increase crop yield is the use of fertilizer subsidies, which can be studied by using an economic model.</p> <p>Quick links</p> <ul> <li>DYNAMO Portal: http://mint.tacc.utexas.edu/</li> </ul>"},{"location":"#capabilities-of-dynamo","title":"Capabilities of DYNAMO","text":"<p>To enable these kinds of explorations, DYNAMO contains a range of hydrologic, agricultural, and economic models as well as a wealth of regional-level data needed to run those models.</p> <p>After selecting a region of the world, the DYNAMO main interface guides an analyst through a series of tasks:</p> <p>Fig. 1: Tasks available in DYNAMO.</p> <ul> <li> <p>Explore Variables. Users can explore all standard variables, their units, and representations. Standard variables are used to tag data and models, allowing users to filter the data each model needs.</p> </li> <li> <p>Explore Areas. Identify geographical areas for modeling. DYNAMO shows pre-defined areas such as river basins for hydrology modeling, cropland areas for agriculture modeling, and administrative regions for economic modeling.</p> </li> <li> <p>Browse and prepare the models available. A user can browse DYNAMO\u2019s Model Catalog to see the models available, their characteristics, their data needs, the processes they represent along with the variables involved in those processes, and the results they produce. The models in DYNAMO have been customized to the pre-defined geographical areas.   Users can create custom configurations to set up parameters, files, and regions, enabling constrained execution of models</p> </li> <li>Use models to run simulations that help answer questions of interest. DYNAMO guides an analyst through necessary steps to select and setup a model thread to run by offering sensible choices based on the analyst\u2019s previous selections and information about the models and data in the Model and Data Catalog. The next section provides more information on this topic.</li> <li>Visualize results of model runs. Once models are run, analysts can visualize the results and include these visualizations as well as their provenance details into reports.</li> <li>Collaborate with other users with different expertise. Analysts can share their modeling results with others, request results from others, and discuss how to coordinate modeling tasks for consistency through a message board.</li> <li>Discover Data. A user can browse the DYNAMO\u2019s Data Catalog to see all the datasets available. DYNAMO is pre-populated with datasets relevant to modeling in the pre-defined regions. DYNAMO highlights special datasets that may be of particular interest, such as high-quality datasets, necessary for the modeling tasks as well as novel datasets extracted from remote sensing data through machine learning techniques. These special datasets allow modeling experts to prepare and customize models for a region, so they have higher accuracy. To know more check the CKAN section. </li> </ul>"},{"location":"#formulating-modeling-problems-and-modeling-tasks-in-dynamo","title":"Formulating Modeling Problems and Modeling Tasks in DYNAMO","text":"<p>Fig. 2: Problem statements in DYNAMO.</p> <p>Analysts frame their problem into problem statements that establish the guiding context and its associated time frame to reflect the planning horizon or time period of interest. For instance, a problem statement can be stated as \u201cFlood Edinburg Hurricane Beulah\u201d to group diffent types of analysis.</p> <p>For each problem statement, analysts can formulate modeling tasks. Each modeling task is associated with an indicator relevant to the decision that want to inform or support.  For example, for problem statement described above, one modeling task can be framed as \u201cDataset Analysis - Sunday Speaker Audio\u201d, and a separate modeling task could be a different day or a different task. Note that the time frame of the tasks does not necessarily reflect that of the problem statement. </p> <p>Fig. 3: Problem statements in DYNAMO.</p> <p>Fig. 4: Task form details.</p> <p>Analysts may want to explore different initial conditions. These are expressed as adjustable parameters and input variables of models. For instance, an analyst can explore different weed management strategies on crop production by specifying different values for adjustable parameter that sets the crop to weeds ratio (or range thereof) in an agriculture model.</p> <p>Note that problem statements and task statements are not processed by DYNAMO, it is simply a starting point for an analysis. An analyst may create several problem statements, each leading to different analyses. In contrast, DYNAMO understands the indicators and modeling variables of modeling tasks, and uses them to guide analysts in finding and setting up appropriate models. Interventions reflect human actions that can change the course of a system\u2019s behavior. They can be explored through the settings of adjustable parameters and input variables. For example, if an agency chooses to establish a fertilizer subsidy to incentivize farmers to plant a particular crop, an analyst can explore these interventions by adjusting a model\u2019s parameter for fertilizer prices. There is documentation about this in the DYNAMO Model Catalog, and in the Task editor when the adjustable variable chosen has an associated intervention.</p> <p>A modeling task can be accomplished through several modeling threads. Separate modeling threads may be created to explore different aspects of the task. For example, a modeling task to explore a crop production index may have two modeling threads, each using a different agriculture model. Separate threads can be created to explore different initial assumptions, or to consider different indices. Each modeling thread is independent of others.</p>"},{"location":"#using-models-in-dynamo","title":"Using Models in DYNAMO","text":"<p>DYNAMO helps analysts to use models to accomplish the modeling tasks through several steps:</p> <ol> <li>Select variables: DYNAMO shows analysts the possible indicators that can be generated based on the models available. The analyst can also select input variables and parameters that they wish to adjust.</li> <li>Select models: DYNAMO then shows analysts the models available that generate the indicators of interest, and that have the adjustable parameters and intervention inputs desired by the analyst. Analysts can compare models and select one or more models to run.</li> <li>Select datasets: DYNAMO then shows analysts the datasets that are available as inputs to the models selected. Analysts can compare datasets and select one or more datasets to run.</li> <li>Set up models: DYNAMO shows analysts the adjustable parameters that are input to the model, and the possible values that they can take. Analysts can select multiple parameter values which result in different runs.</li> <li>Monitor the status of model runs: This allows analysts to track model executions that take a long time, and to be informed of execution failures.</li> <li>View results of model executions: Analysts can download and save any results from models.</li> <li>Visualize model results: DYNAMO generates interactive visualizations that allow analysts to understand the model results.</li> </ol> <p>When a step has been completed, it is shown in a darker color. Users can revisit an earlier step, and if the choices are changed for that step then the subsequent steps are canceled and need to be redone.</p> <p>Each of these steps is done in a separate Web page in DYNAMO, and has its own URL. This is very convenient to point other analysts to a particular selection or result by sharing its URL through the message board.</p>"},{"location":"#glossary","title":"Glossary","text":"<ul> <li>Adjustable parameter: A parameter whose value affects an input variable of a model. For instance, an analyst can explore different weed management strategies on crop production by specifying different values for adjustable parameter that sets the crop to weeds ratio (or range thereof) in an agriculture model.</li> <li>Driver: An input dataset that creates initial conditions for a model. For example, a weather forecast. Index: Combination of 2 or more variables that can be measured (any of which could be seen as indicators) with the goal of using a single number for assessment and comparison purposes.</li> <li>Indicator: A quantifiable variable that is identified as playing a special role, namely to help characterize a complex property of a system being modeled. Indicators can be single variables or combinations of variables (called indices). Input variable: A variable that will influence a response in the system, and is input to a model.</li> <li>Modeling task: A modeling task is accomplished through a series of modeling runs in order to answer a question of interest.</li> <li>Modeling thread: A modeling thread groups together modeling runs that are conceptually related. Modeling problem: A text statement that describes what an analyst wants to study in a system over a specific time period. The statement is not machine readable, it is simply a mechanism for analyst to organize their investigations.</li> <li>System: A physical system under study, where models capture important processes or aspects of the system so its response to input variables can be studied.</li> </ul>"},{"location":"#frequently-asked-questions-faqs","title":"Frequently Asked Questions (FAQs)","text":"<p>Q1: How should I formulate problem statement?</p> <p>A1: Problem statements are high-level problems set by the decision maker. They need to be broad enough to encompass a problem of interest.</p> <p>Q2: How should I formulate modeling tasks?</p> <p>A2: Modeling tasks can be formulated in any way that the analyst finds useful to decompose a problem statement into tractable modeling tasks. For example, a separate modeling task should be created for each indicator that needs to be generated to support the problem statement. A good practice is that each modeling task concerns only one model or discipline, over a subregion appropriate for the model.</p> <p>Q3: How should I use modeling threads?</p> <p>A3: Separate modeling threads can be used to explore alternative models (i.e. run Model 2 and then run Model 1 to see if the results are consistent and therefore there should be higher confidence in the predicted outcomes), explore alternative values for input variables and adjustable parameters, and use different input data.</p> <p>Q4: Can I point another analyst to a result from my own work?</p> <p>A4: Yes. In the message board, any message may include the URL of any page in the DYNAMO interface. Simply cut and paste the URL of that page that contains the result or step that you want another analyst to see.</p> <p>Q5: Can I run more than one model within a single modeling thread?</p> <p>A5: This may come up when the output from a model may be needed as input to another model. For example, modeling effective crop production using an economic model requires understanding how a crop responds to changes in fertilizer rates using an agriculture model. In some cases, each model may be run by a different analyst, but the analysts may need to coordinate the parameters and input variables for both models to be consistent. In DYNAMO, each model would be part of a different modeling task or thread, and the analysts would need to communicate with one another through the message board. For example, an analyst can start a modeling thread and send the URL of that thread via the message board to another analyst asking them to do the modeling. The second analyst can share the results through a URL in a message.</p> <p>Q6: Do I need to upload data to DYNAMO?</p> <p>A6: This is not necessary. DYNAMO already contains many relevant datasets for the regions of interest. Check the CKAN section to learn more.</p> <p>Q7: Do I need to add new models in DYNAMO?</p> <p>A7: No. DYNAMO is pre-populated with models for the regions of interest. In the future, DYNAMO will include capabilities for modeling experts to add new models.</p>"},{"location":"datacatalog/","title":"MINT Data Catalog","text":""},{"location":"datacatalog/#main-website","title":"Main website","text":"<p>https://data-catalog.mint.isi.edu</p>"},{"location":"datacatalog/#summary","title":"Summary","text":"<p>The goal of MINT Data Catalog is to provide the data you need in the format you want. The high-level functionality can summarized with the following image</p> <p></p> <p>More formally, the MINT Data Catalog is a system that provides a curated collection of datasets. Each dataset is a logical grouping of data about specific variables contained in one or more resources. The data in a dataset share metadata such as geospatial and temporal extent and provenance. Each dataset contains information about one or more variables, or scientific quantities of interest with a precise ontological definition. Variables are associated with one or more standard variable names, which are ontological classes in ontologies defined by domain scientists. Often a standard name is not sufficient to fully describe all of the metadata about a variable, so the data catalog defines a variable presentation capturing information about variables in datasets. Variable presentations include information about the variable\u2019s representation such as the units of measure, handling of missing values, and metadata about collection. Data is physically located in one or more resources indexed by the data catalog. A resource can be a physical file, a web resource, or an API endpoint. For each resource, we define a layout, which captures the physical relationships between variables in the resource. For example, in a CSV file with columns corresponding to months and rows corresponding to different variables of interest (e.g., GDP, inflation rate, imports, exports, etc.), the layout specifies which row contains each variable and how those variables relate to the columns (time), while the variable presentation provides metadata such as units and how the variables were measured. </p> <p>For more information, please visit https://github.com/mintproject/MINT-DataCatalog-Public</p>"},{"location":"datacatalog/#mint-data-catalog-api","title":"MINT Data Catalog API","text":"<p>Many datasets that are used in MINT contain thousands or even tens of thousands of resources. Because of this, Data Catalog provides an API in order to facilitate programmatic interactivity. </p> <ul> <li> <p>The API is currently deployed at: https://api.mint-data-catalog.org/</p> </li> <li> <p>API Documentation can be found at https://data-catalog.mint.isi.edu/documentation</p> </li> <li> <p>There is an interactive Jupyter Notebook that can be used to play around with various API calls in a sandboxed environment (without affecting existing data)</p> </li> </ul>"},{"location":"indicators/","title":"An Overview of Indicators Generated by MINT","text":"<p>The rest of this document gives an overview of the indicators for each subject, and their importance for the region.</p>"},{"location":"indicators/#agriculture-indicators","title":"Agriculture Indicators","text":"<p>The Potential Crop Production is generated using the Cycles model, which reports outputs for maize, sorghum, sesame and peanuts for all of South Sudan. The focus of these simulations is the first growing season (planting March through May), which leads to the late June through August harvest. The crop yields and other variables reported, correspond spatially to a climate grid (4 x 4 km) covering all of South Sudan. At each grid point we simulated a full combinatorial matrix of 7 planting dates (from early to late planting), 5 nitrogen fertilization rates and 3 weed pressure levels. That provides a full range of potential situations that should encompass production conditions. We provide the model outputs, which are admittedly overwhelming and we are working on visualizations to facilitate understanding.</p> <p>The Seasonal Crop Production Index also generated using the Cycles model, provides a fast and robust way of reporting normalized yields for a given grid point or region. The goal of the index is to easily segment \u201cyears\u201d or \u201cseasons\u201d that can be problematic for food production, and to provide an intuitive sense of the magnitude of the problem. While the raw yield outputs might be difficult to interpret for the non-expert, the index is more transparent. It is based on the ratio of the yield of a given combination of planting date, fertilization and weed pressure to the median yield under such conditions across years. An index = 1 means that the year resembles the median years, and it is assumed that it represents a neutral condition (neither local excess or shortage of food supply). An index &gt; 1 means that the local food supply may exceed the median, and the larger the number the better off the location. The caveat here is that flooding damage might be underestimated. As we refine our simulations this limitation will be lifted. An index &lt; 1 means a situation worse than normal, and the lower the value the worse the situation. It is safe to assume that this is a reliable indicator of local food shortage. We are working on visualizations that map this index, to give an analyst a sense of the geographical variation of the index. The figure below shows a summary for this index for maize for one grid point in the Pongo Basin.</p> <p>What additional information can complement this raw index? It would be important to have an indicator of both the technology level of a given system and the dependence of a region on the season\u2019s harvest. The higher the technology and the lesser the dependence, the lesser the risk of an index below 1, and conversely, the closer to technology to low-input subsistence agriculture and the higher the dependence on local food supply the more severe the risk. In other words, the index may draw the eyes quickly to the areas at risk (index &lt; 1), but context is needed for a more granular interpretation.</p> <p>Figure 1 shows the Variation of the Seasonal Crop Production Index for maize for one agricultural grid point in the Pongo Basin. The nitrogen rates represent from low to high fertilization (for the agronomy minded, 78 to 1250 kg/ha of raw fertilizer with 32% nitrogen). The 0.4 in the legend indicates the weed pressure (medium). The planting dates are not shown here, but an analyst can easily explore their impact by expanding the data selection. Clearly, years 2011, 2016 and 2017 in particular were problematic.</p> <p>Fig. 1: Variation of the Seasonal Crop Production Index for maize for one agricultural grid point in the Pongo Basin.</p> <p>Crop Production is estimated using an economic model of decisions by agricultural households about the use of inputs (e.g., land and fertilizer) to produce a set of crops currently grown in the study region. The economic model consists of a numerical simulation of a non-linear constrained optimization problem that is calibrated to reflect observed decision-making in the study region. The calibration approach uses Positive Mathematical Programming (PMP), which ensures that the simulation model reflects unobserved constraints (e.g., access to credit, labor availability, crop rotation practices) that affect decisions about agricultural production. By varying related parameters (prices, land/fertilizer cost) one at a time between -50 and 50 (increment by 10) for each crop (cassava, sorghum, maize, groundnuts, and sesame) within MINT, simulated crop production results are generated to predict how farmers react to potential economic condition changes.</p> <p>The model is aggregate and represents decision-making by all agricultural households in the study region. The model predicts agricultural production, taking into account the behavioral response of farming households to changes in environmental, economic, and policy drivers. Output variables include the total amount of land in agricultural production, the amount of land and fertilizer used per unit area in the cultivation of each of the region\u2019s crops, average crop yields, and net revenues earned in agricultural production. For the Pongo Basin in South Sudan, the model generates these output variables for maize, cassava, sorghum, groundnuts, and sesame seed crops.</p>"},{"location":"indicators/#hydrology-indicators","title":"Hydrology Indicators","text":"<p>We use the PIHM and TopoFlow models to generate hydrology indicators. These models require a large number of spatially-distributed input variables that describe various properties of the topography (e.g. elevation, slope, flow direction, total contributing area), the meteorology (rainfall rate, relative humidity, air temperature, surface temperature, etc.) and the soil (including many intrinsic and hydraulic properties). The models also require information about the bankfull widths, depths and bed roughness of all the channels within the river networks themselves, which are parameterized with empirical formulas.</p> <p>Geospatial Flood Exceedance index are generated using the PIHM hydrological model. PIHM uses results from a large number of runs over time to compute statistical indexes that characterize the likelihood and potential magnitude of flooding at individual locations in a region (i.e. for every model grid cell). In this case the inundation or flooding potential is determined for 2017 as the simulated average depth of water over a given month with higher values indicating a greater likelihood of flooding. In the long term this index will be based on the full historical record allowing a probabilistic measure of flooding.</p> <p>We are focused on modeling the hydrologic response to individual storms. This means computing geospatial grids that vary in time (like a movie), for the channel depths, flood depths, velocities and discharges of rivers. This type of output shows the areas where flooding occurs, as well as how long before the flooding subsides and many other related aspects of river response to given storms or droughts. We provide raw model output, as well as the data we used for the visualization below.</p> <p>Figure 2 shows this index for the Lol-Kuru rivers with the outlet defined at the location just upstream of where the Pongo river enters the Lol. The visualization shows the parts of the basin where the model points to areas that are most likely candidates for flooding, crop disruption and potential migration. Higher values have a higher likelihood of flooding. In this example the sample size is too small to be reliable but future analyses will use several decades of simulation results to improve the reliability of the results including ensemble simulations.</p> <p>Fig. 2: Snapshots of the Geospatial Flood Exceedance index for Lol-Kuru (Pongo) region show that different areas are affected over time. A movie is available in the MINT indicators spreadsheet.</p> <p>The Streamflow - Duration Index (SDI), also called a flow-duration-curve (FDC), represents the relationship between the magnitude and frequency of daily streamflow for a particular river basin, and provides an estimate of the percentage of time a given streamflow was equaled or exceeded over the historical record of the data. An SDI provides a simple, yet comprehensive, graphical view of the overall historical variability associated with streamflow in a river basin. SDI is the complement of the cumulative distribution function (CDF) of daily streamflow. Each value of discharge Q has a corresponding exceedance probability, and an SDI is simply a plot Qpversus the pthquantile or percentile of daily streamflow versus exceedance probability p = 1-Prob[Q&lt;= Qp].</p> <p>The SDI was computed for the simulated Lol-Kuru rivers with the outlet defined at the location just upstream of where the Pongo river enters the Lol. The simulation was for 2001-2017 daily climate data. The model was manually calibrated and inputs and outputs of the simulation can be found in the table.</p> <p>Figure 3 below is the simulated runoff at the basin outlet and Figure 4 is the SDI for the basin. The daily streamflow record mirrors the seasonality of rainfall, although the peak or maximum annual daily flows show extreme fluctuations in both magnitude and frequency of occurrence. The impact of frequency-magnitude variability on food security is that droughts and floods tend to occur in clusters with memory or duration lasting several years. In addition the question of non-stationarity or time varying statistics may have to be addressed in future analyses.</p> <p>Fig. 3: The 2001-2017 daily streamflow in the Lol-Kuru catchment (Pongo), used to compute the SDI indicator</p> <p>Fig. 4: The Streamflow duration index (SDI) for the Lol-Kuru catchment (Pongo). The SDI represents the magnitude-frequency for historical daily streamflow. Annual average flows less than the median (0.5 Exceedance) represent likely drought conditions while annual average flow greater than .001 exceedance frequency are likely associated with flooding conditions.</p> <p>We report the River Discharge and River Flood Depth computed with the TopoFlow hydrologic model for the Baro River basin draining to the town of Gambela, Ethiopia. The spatial extent of the basin in shown in Figure 5.</p> <p>Fig. 5: Shaded relief image for the Baro River basin draining to the town of Gambela, Ethiopia, with overlaid basin boundary and extracted channel network. The spatial extent of this image matches that of the 2 movies for River Discharge and River Flood Depth, with links in the spreadsheet, computed with the TopoFlow hydrologic model.</p>"},{"location":"interventions/","title":"Overview of Interventions Currently Supported in MINT","text":"<p>Interventions in MINT reflect human actions that can change the course of a system\u2019s behavior. Currently, the models in MINT supports an analyst to explore three kinds of interventions: fertilizer subsidies, planting windows, and weed control methods.</p> <p>In MINT, an intervention is linked to an adjustable parameter of a model. The table below summarizes the interventions that MINT currently supports.</p> Intervention Description Model Adjustable Parameter(s) Fertilizer subsidies Interventions concerning fertilizer subsidies can be expressed in this model as a percentage of fertilizer prices EACS <code>sesame-fertilizer-cost-adjustment</code>, <code>cassava-fertilizer-cost-adjustment</code>, <code>maize-fertilizer-cost-adjustment</code>, <code>sorghum-fertilizer-cost-adjustment</code>, <code>groundnuts-fertilizer-cost-adjustment</code> Planting windows Interventions that force specific target planting windows can be expressed in this model as start and end planting dates Cycles <code>start-planting-day</code>, <code>end-planting-day</code> Weed control Interventions concerning weed control and weed management practices can be reflected in this model by indicating the fraction of weeds that will remain after the weed treatments applied by farmers Cycles <code>weed-fraction</code> <p>There are two specific places in the MINT user interface where interventions can be seen:</p> <ul> <li>The MINT Model Catalog. There is documentation about interventions under the Parameters and Inputs tab. The parameters table has a column called \u201cRelevant Interventions\u201d, and hovering over the intervention shows its description.</li> <li>The Modeling Task Editor. When an adjustable variable is chosen that has an associated intervention there is a description of the intervention shown.</li> <li>The Setup step in the Use Models tab. When the adjustable parameters are edited, the documentation shows when an intervention can be considered.</li> </ul> <p>For example, an intervention that could be explored is providing fertilizer subsidies to farmers. This can be explored with the EACS economic model by adjusting the parameters concerning the cost of fertilizers for each crop type. The maize fertilizer price adjustment parameter reflect the percentage change in the unit cost of nitrogen fertilizer as an input into the production of maize, where a reduction in the unit cost of nitrogen fertilizer for maize can be interpreted as a fertilizer subsidy for maize. The range of this parameter is from -50 to 50. So a value of this parameter of -10 reflects a moderate subsidy. The model takes into account farmer\u2019s behaviors when fertilizer cost is lower, and will adjust crop production accordingly.</p>"},{"location":"modelcatalog/","title":"A Quick Guide to the DYNAMO Model Services","text":""},{"location":"modelcatalog/#overview","title":"Overview","text":"<p>The DYNAMO Model Services describe physical, environmental and social models (e.g., climate, hydrology, agriculture or economy models) in order to:</p> <ul> <li>Find relevant models, according their name, keywords, variables or region where they can be executed.</li> <li>Execute models, either by providing new input files and parameters or by choosing pre-selected models that have been setup by experts.  </li> <li>Understand how to use models and how to interpret their results. In order to prepare input data for model execution and ensure that users can use model results, the most relevant variables included in the input and output files should be described. Similarly, other geospatial and temporal information is critical for providing the context key to understand model results.</li> </ul> <p>Figure 1 shows an overview of the main categories we use to describe models, which are further described in the following sections.</p> <p>Fig. 1: Overview of the Model Services' main categories  for finding, executing, and understanding models.</p> <p>Quick links</p> <ul> <li>GUI for exploring the contents of the model catalog: https://models.mint.isi.edu/home. (See a video of the main features)</li> <li>REST API for adding/modifying/deleting model metadata: https://api.models.mint.isi.edu/latest</li> <li>[Requires log in] GUI for configuring and editing models: https://mint.isi.edu/ethiopia/models/configure</li> <li>Model services API client and examples: https://model-catalog-python-api-client.readthedocs.io/en/latest/</li> <li>Model catalog API documentation: https://model-catalog-python-api-client.readthedocs.io/en/latest/endpoints/</li> </ul>"},{"location":"modelcatalog/#making-your-model-findable","title":"Making your model findable","text":"<p>This level provides a basic description of a model. At this stage, models do not need to be executable in order to be included in the model catalog. Making a model findable ensures that it has minimum metadata to describe its attribution, proper citation, and (if they exist) pointers to the website, creator, license, and maintainer in addition to relevant information to modelers such as a description, basic keywords and relevant assumptions.</p>"},{"location":"modelcatalog/#model-and-model-versions","title":"Model and model versions","text":"<p>We include two main levels of granularity for making models findable. The first involves describing the model itself, at a generic level. The second level describes the model version(s) available in the catalog. Versions do not necessarily supersede each other as models under continuous development may have multiple co-existing versions with different characteristics serving different purposes (see Figure 2).</p> <p>Fig. 2: Capturing models and model versions. The MODFLOW groundwater model has two versions, one for official USGS use and the other for a specific modeling problem. Both versions are currently in use.</p>"},{"location":"modelcatalog/#making-your-model-executable","title":"Making your model executable","text":"<p>The main driver for the model catalog is to make the model executable with different input variable and parameter values. For instance, the particular configuration of the Topoflow hydrology model shown in Figure 3 allows to manipulate two input files (precipitation and configuration files to turn on the infiltration process) and two parameters (hydrologic conductivity and the simulation years to run the model), which describe numerical options for the model:</p> <p></p> <p>Fig. 3: TopoFlow model configuration with 2 input files, 2 input parameters, and 2 outputs.</p> <p>We may want to execute this model in a region with a certain  configuration file and precipitation rates, or we may want allow users to change both of these files to files they have created themselves. In order to support this flexibility, we distinguish among the following concepts for executing models:</p>"},{"location":"modelcatalog/#model-configuration","title":"Model configuration","text":"<p>Represents a unique way of running a model, exposing concrete inputs, outputs, and parameters. Here we refer to model inputs/outputs as the data types that are used for executing the model rather than pointing to specific files. For instance, in Figure 3 the precipitation rates is a CSV file with the information about the rain amount in an area. We use the term parameters to refer to those values a user may be interested in changing in a model (e.g., the hydrological conductivity or the simulation years in Figure 3).</p> <p>The same version of a model may have different model configurations, which expose different functionalities. For example, TopoFlow may have another model configuration without considering infiltration, which returns different results from the one in Figure 3.</p>"},{"location":"modelcatalog/#model-configuration-setup","title":"Model configuration setup","text":"<p>Model configurations may have multiple parameters aimed at expert users that are too complicated and not necessarily useful to users outside the domain of expertise. A model configuration setup represents a layer of abstraction over a model configuration, simplifying it and making it easier to execute in a correct and meaningful manner. Using the example in Figure 3, let's create a model configuration setup for a particular region:</p> <p>Fig. 4: Main differences between model configuration and model configuration setup. While a model configuration exposes all the files and parameters needed to execute a model (2 files and 2 parameters on the left), the model configuration setup on the right simplify them (only precipitation rates and simulation years are exposed).</p> <p>As shown in Figure 4 (right), the model configuration setup has been adjusted with the input file with infiltration to a fixed URL by an expert modeler and with a fixed hydrologic conductivity value reasonable for the study region. To execute the setup, users would only have to select the precipitation files and simulation years.</p> <p>Setups may have all input files initialized by expert users, allowing variation only in the parameters that may be interesting in a region. This way users can execute model ensembles of interest without worrying about complex input file selection and preparation that may not be relevant to their task.</p> <p>When should you use model configurations versus model configuration setups?</p> <p>An executable model with a fixed subset of inputs is a model configuration setup. This is extremely useful to fix values for variables and parameters that (1) are invariant, (2) require lots of expertise to set correctly, (3) may not be relevant to the problem at hand.</p>"},{"location":"modelcatalog/#making-your-model-understandable","title":"Making your model understandable","text":"<p>This last level ensures that the details about the model are easy to understand by others. This task requires providing metadata in the following categories:</p>"},{"location":"modelcatalog/#model-inputs-outputs-and-parameters","title":"Model inputs, outputs, and parameters","text":"<p>Because of the level of abstraction desired for model configurations and setups, they may not sufficiently describe a model for other researchers with expertise in the domain to understand and re-use them. For each input (including parameters) and output, we recommend providing a short description on how they affect the behavior of the model. For parameters, it is also necessary to describe their default values and minimum and maximum values.</p>"},{"location":"modelcatalog/#variables-and-units","title":"Variables and Units","text":"<p>Variables for each input and output datasets used in configurations should be described in as many details as possible to help with data preparation. Units are often the most important piece of information when dealing with data transformation.</p> <p>While it is not required to describe all input/output variables in a model, we recommend to describing at least all those that are critical for selecting data (e.g., precipitation variable in the Precipitation rates file of Figure 3) and plotting results.</p>"},{"location":"modelcatalog/#grid","title":"Grid","text":"<p>Models with a geospatial grid (e.g., hydrology) should describe the characteristics of the grid (point based, 2-D, irregular, etc.) so as help determine whether regridding of the inputs/outputs is necessary. In addition, providing the coordinate projection used in the input/output data is required to know how to project it in a map.</p>"},{"location":"modelcatalog/#region","title":"Region","text":"<p>Model configurations and setups are usually calibrated or configured by experts (manually or automatically) to be run in a specific region. Hence, describing the region in which the model has been prepared to run is required to understand the context of the obtained results. For example, the hydrology model shown in Figure 3 is may configured differently in regions rich in clay, with little or no infiltration possible.</p>"},{"location":"modelcatalog/#time-step","title":"Time Step","text":"<p>Different models output data at different time steps, usually depending on the data used as input. This information is necessary for data transformation and for linking models with different time requirements (e.g., a hydrology model may become unstable if run every month but monthly time steps are needed to initialize soil moisture in an agriculture model.)</p>"},{"location":"modelcatalog/#usage-examples-and-tutorial","title":"Usage Examples and Tutorial","text":"<p>We have prepared a set of materials to help illustrating how the model catalog API and client work.</p> <ol> <li>Step by step example on how to search models (e.g., by keyword) with the model catalog API: https://model-catalog-python-api-client.readthedocs.io/en/latest/models/</li> <li>Step by step example on how to retrieve the available versions of a model: https://model-catalog-python-api-client.readthedocs.io/en/latest/modelversion/</li> <li>Step by step example on how to find executable model configurations of a model: https://model-catalog-python-api-client.readthedocs.io/en/latest/modelconfigurations/</li> <li>Examples on how to execute models in the model catalog with a command line client: https://model-catalog-python-api-client.readthedocs.io/en/latest/example/</li> <li>Overview video of the main capabilities of the model catalog (link to video). A simplified overview can be seen below:</li> </ol> <p></p>"},{"location":"modelcatalog/#status","title":"Status","text":"<p>The current model catalog API (v1.8.0, released on Oct 21th, 2022) supports:</p> <ul> <li>Retrieving information from models [GUI, API, API client]</li> <li>Edit information from model setups [GUI, API] (needs authentication)</li> <li>Adding new model metadata [API] (needs authentication)</li> <li>Current schema (draft) for representing model metadata current schema draft</li> <li>Command line interface for guiding, validating and helping users adding executable models (currently under development)</li> </ul>"},{"location":"modelcatalog/#next-steps","title":"Next steps","text":"<p>We are currently improving the model services to support the following capabilities:</p> <ul> <li>[March 30] Desktop Application for Model Execution (DAME) for running pre-configured hydrology models locally/on a server </li> <li>[April 15] Simple model insertion for running model executables.</li> <li>[May 30] Integrate data transformations for hydrology models supported.</li> <li>[June 30] Model insertion through guidebook for specifying compact metadata documentation in stages of progressive detail.</li> <li>[July - August] Improve integration and transition.</li> </ul> <p>Figure 5 shows an overview of the components that are part of the MINT model services and the new planned enhancements. </p> <p>Fig. 5: Overview of MINT Model Services.</p>"},{"location":"modeling/","title":"MINT Modeling Status","text":""},{"location":"modeling/#south-sudan","title":"South Sudan","text":""},{"location":"modeling/#backcasting","title":"Backcasting","text":"<p>Regions: Focus on Pongo region (other regions by first approximation based on Pongo)</p> <p>Time period supported: any period between 2000-2017</p> <p>Agriculture modeling for potential crop production (Cycles model):</p> <ul> <li>Model is manually tuned for agriculture areas within Pongo region</li> <li>For other regions in South Sudan, the model can be tuned by hand and used</li> </ul> <p>Economic modeling for crop production (EACS model):</p> <ul> <li>Model is calibrated for areas within the Pongo region</li> <li>For other regions of South Sudan, the Pongo calibration of the model could be used as a first approximation</li> </ul> <p>Hydrology modeling for flooding (PIHM model):</p> <ul> <li>Only available as an emulator, for Pongo region only</li> </ul>"},{"location":"modeling/#forecasting","title":"Forecasting","text":"<p>Regions: Focus on Pongo region (other regions by first approximation based on Pongo)</p> <p>Time period supported: 2018 (weather forecasting available for different percentiles). Forecasting is based on historical precipitation within Pongo Basin. A primer on forecasting in MINT is available here.</p> <p>Agriculture modeling for crop yield (Cycles model):</p> <ul> <li>Model is manually tuned for agriculture areas within Pongo region</li> <li>For other regions in South Sudan, the model can be tuned by hand and used</li> </ul> <p>Economic modeling for crop production (EACS model):</p> <ul> <li>Model is calibrated for areas within the Pongo region</li> <li>For other regions of South Sudan, the Pongo calibration of the model could be used as a first approximation</li> </ul> <p>Hydrology modeling: Not available for forecasting.</p>"},{"location":"modeling/#ethiopia","title":"Ethiopia","text":""},{"location":"modeling/#backcasting_1","title":"Backcasting","text":"<p>Regions: Focus on Gambella region and Baro river (other regions by first approximation)</p> <p>Time period supported: any period between 2000-2017</p> <p>Agriculture modeling for crop yield (Cycles model):</p> <ul> <li>Model is tuned for agriculture areas within the Gambella region</li> <li>For other regions in Ethiopia, the model can be calibrated by hand and used</li> </ul> <p>Economic modeling for crop production (EACS model):</p> <ul> <li>Model is calibrated for areas within the Gambella region</li> <li>For other regions of Ethiopia, the Gambella calibration of the model could be used as a first approximation</li> </ul> <p>Hydrology modeling for river discharge/flooding (PIHM model):</p> <ul> <li>Model is available for Baro region only</li> </ul>"},{"location":"modeling/#forecasting_1","title":"Forecasting","text":"<p>Regions: Focus on Gambella region and Baro river (other regions by first approximation)</p> <p>Time period supported: Time period supported: 2018 (weather forecasting available for different percentiles). Forecasting is based on historical precipitation within Baro Basin. A primer on forecasting in MINT is available here.</p> <p>Agriculture modeling for crop yield (Cycles model):</p> <ul> <li>Model is manually tuned for agriculture areas within the Gambella region</li> <li>For other regions in Ethiopia, the model can be calibrated by hand and used</li> </ul> <p>Economic modeling for crop production (EACS model):</p> <ul> <li>Model is calibrated for areas within the Gambella region</li> <li>For other regions of Ethiopia, the Gambella calibration of the model could be used as a first approximation</li> </ul> <p>Hydrology modeling for river discharge/flooding (TopoFlow model):</p> <ul> <li>Model is available for Baro region only</li> </ul>"},{"location":"solutioncatalog/","title":"Solution Catalog","text":""},{"location":"solutioncatalog/#overview","title":"Overview","text":"<p>Once we execute a model or model ensemble in MINT, we save all results in the MINT solution catalog. The model information used in the execution is also saved, along with additional metadata with the provenance of the final results (which parameter values were set, which inputs were used, which executions were successful or not, etc.)</p> <p>The solution catalog is accessible as a database that can be browsed, allowing users and developers to query for results that already exist. The following python notebook shows an example on how to access, browse, and filter results:</p> <ul> <li>Notebook for browsing execution results</li> </ul>"},{"location":"walkthrough/","title":"DYNAMO User Interface Walkthrough","text":""},{"location":"walkthrough/#log-in-and-region-overview","title":"Log in and region overview","text":"<p>Let's start loging in and selecting a region of interest. </p> <p>Once you are logged in and have selected a main region, the top menu will show you new options. </p> <p>The top menu gives you access to the main sections:</p> <ul> <li>Explore Areas</li> <li>Prepare Models</li> <li>Browse Datasets</li> <li>Use Models</li> <li>Prepare Reports</li> </ul>"},{"location":"admin-guide/","title":"Overview","text":"<p>This guide is for Administrators who need to set up a new DYNAMO installation.</p> <p>Note</p> <p>Please make sure you have completed the User guide  before you start.</p>"},{"location":"admin-guide/authentication/","title":"Authentication","text":"<p>DYNAMO supports the OAuth 2.0 protocol for authorization. The authorization code grant, password grant, and implicit flow are supported.</p>"},{"location":"admin-guide/authentication/#configuration","title":"Configuration","text":"<p>OAuth 2.0 must be configured in the DYNAMO-UI <code>config.js</code> file. The following variables must be set:</p> <ul> <li><code>REACT_APP_AUTH_GRANT</code>: Use <code>password</code> for the password grant, <code>implicit</code> for the implicit flow and <code>code</code> for the authorization code.</li> <li><code>REACT_APP_AUTH_SERVER</code>: The base URI for the authentication server.</li> <li><code>REACT_APP_AUTH_CLIENT_ID</code>: The client ID associated with DYNAMO on the authentication server.</li> <li><code>REACT_APP_AUTH_TOKEN_URL</code>: The path to the token API on the authentication server.</li> <li><code>REACT_APP_AUTH_AUTH_URL</code>: The path to the authentication API on the authentication server.</li> <li><code>REACT_APP_AUTH_DISCOVERY_URL</code>: The path to the discovery API on the authentication server.</li> <li><code>REACT_APP_AUTH_LOGOUT</code>: The path to the logout or revoke API on the authentication server.</li> </ul> <p>Optional variables:</p> <ul> <li><code>REACT_APP_AUTH_PROVIDER</code>: For custom implementations, use <code>tapis</code> for Tapis authentication servers or <code>keycloak</code> for Keycloak authentication servers.</li> <li><code>REACT_APP_AUTH_HASH</code>: The hash for basic authentication. Will be written in the headers as <code>Authorization: Basic &lt;HASH&gt;</code>. This hash value can be generated in the browser using <code>btoa(&lt;username&gt;:&lt;password&gt;)</code>.</li> </ul>"},{"location":"admin-guide/authentication/#example","title":"Example","text":"<pre><code>window.REACT_APP_AUTH_GRANT = \"code\";\nwindow.REACT_APP_AUTH_PROVIDER = \"keycloak\";\nwindow.REACT_APP_AUTH_SERVER = \"https://auth.mint.tacc.utexas.edu\";\nwindow.REACT_APP_AUTH_CLIENT_ID = \"dynamo-ui\";\nwindow.REACT_APP_AUTH_TOKEN_URL = '/realms/production/protocol/openid-connect/token';\nwindow.REACT_APP_AUTH_AUTH_URL = '/realms/production/protocol/openid-connect/auth';\nwindow.REACT_APP_AUTH_DISCOVERY_URL = '/realms/production/.well-known/openid-configuration';\nwindow.REACT_APP_AUTH_LOGOUT = '/realms/master/protocol/openid-connect/logout';\n</code></pre>"},{"location":"admin-guide/ckan-configuration/","title":"CKAN configuration","text":"<p>MINT supports CKAN as a data catalog. CKAN is a powerful data management system that makes data accessible by providing tools to streamline publishing, sharing, finding, and using data. CKAN is aimed at data publishers (national and regional governments, companies, and organizations) wanting to make their data open and available.</p>"},{"location":"admin-guide/ckan-configuration/#features","title":"Features","text":"<p>MINT CKAN provides the following features:</p> <ul> <li>Query and search for datasets</li> <li>View dataset metadata</li> <li>Publish datasets and resources from the execution of a workflow</li> </ul>"},{"location":"admin-guide/ckan-configuration/#configuration","title":"Configuration","text":"<p>The CKAN configuration is stored in the <code>values.yaml</code> file in the <code>mint</code> directory. The <code>values.yaml</code> file contains the following sections:</p> <pre><code>external_services:\n  ckan:\n    # -- Enable or disable CKAN service\n    enabled: true\n    # -- CKAN url\n    url: \"http://localhost:5000\"\n    # -- CKAN service type\n    type: \"CKAN\"\n    extra:\n      # -- Owner organization ID for CKAN service\n      owner_organization_id: \"\"\n      # -- Owner provenance ID for CKAN service\n      owner_provenance_id: \"\"``\nsecrets:\n  external_services:\n    ckan:\n      # -- API key for CKAN service. Used by Ensemble Manager to upload data\n      api_key: CHANGEME\n</code></pre>"},{"location":"admin-guide/installation/","title":"Install","text":"<p>MINT can be easily deployed on large Kubernetes clusters using Helm. Useful for production instances.</p>"},{"location":"admin-guide/installation/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes <code>v1.16.3</code> cluster or later.</li> <li>Helm <code>v3.2.x</code> or later.</li> </ul> <p>If you don't have a Kubernetes cluster and you want to test MINT, we strongly recommend to use Microk8s to create a local cluster on your machine. Follow the instructions in the MINT to install Microk8s on your machine.</p>"},{"location":"admin-guide/installation/#install-mint","title":"Install MINT","text":"<p>Add the MINT Helm repository:</p> <pre><code>$ helm repo add mint https://mintproject.github.io/mint\n</code></pre> <p>Update the Helm repository:</p> <pre><code>$ helm repo update\n</code></pre> <p>Install MINT:</p> <pre><code>$ helm install mint mint/mint --namespace mint --create-namespace\n</code></pre> <p>If you using MacOS Silicon, you need to install the <code>arm64</code> version of the postgresql database.</p> <p>WARNING: The arm64 image has not been tested and may not work as expected. Please use it at your own risk.</p> <pre><code>$ helm install mint mint/mint --namespace mint --create-namespace --set arm_support=true\n</code></pre> <p>Helm will returns the URL to access the MINT services. You can use the following command to get the URL:</p> <pre><code>The MINT system has been installed!\n\nPlease remember to edit your `/etc/hosts/`\n\nAfter that, you can access to the MINT services\n\n\nhttp://mint.local\nhttp://ensemble-manager.mint.local\nhttp://cromo.mint.local\nhttp://mic.mint.local\nhttp://datacatalog.mint.local\n</code></pre> <p>If you are using microk8s on a VM (macOS or windows) need to get the IP address of the VM to access the MINT services. You can use the following command to get the IP address of the VM:</p> <pre><code>$ kubectl get node -o json | jq '.items[].status.addresses[] | select(.type==\"InternalIP\") | .address'\n10.211.59.16\n</code></pre> <p>Then, edit the <code>/etc/hosts</code> file and add the following lines:</p> <pre><code>$ sudo vim /etc/hosts\n</code></pre> <p>Add the following lines (replace the IP address with the IP address of the VM):</p> <pre><code>10.211.59.16 mint.local\n10.211.59.16 cromo.mint.local\n10.211.59.16 mic.mint.local\n10.211.59.16 api.mic.mint.local\n10.211.59.16 datacatalog.mint.local\n10.211.59.16 endpoint.models.mint.local\n10.211.59.16 graphql.mint.local\n10.211.59.16 models.mint.local\n10.211.59.16 api.models.mint.local\n10.211.59.16 endpoint.models.mint.local\n</code></pre> <p>Got to the URL returned by Helm to access the MINT services.</p> <p>The default username and password are:</p> <ul> <li>Username: <code>mint@isi.edu</code></li> <li>Password: <code>mint123!</code></li> </ul>"},{"location":"admin-guide/catalog-configuration/model-catalog-configuration/","title":"Model Catalog","text":"<p>The Model Catalog Endpoint is a SPARQL endpoint that provides access to the model catalog data. The Model Catalog Endpoint is implemented using Apache Jena Fuseki. The Model Catalog Endpoint is used by the MINT platform to store and retrieve model metadata.</p> <p>The Model Catalog Endpoint is password protected. The default username and password are <code>admin</code> and <code>CHANGEME</code>, respectively. You can change the username and password by editing the <code>values.yaml</code> file in the <code>mint</code> directory. The <code>values.yaml</code> file contains the following sections:</p> <p>The default url is http://endpoint.models.mint.local</p> <pre><code>secrets:\n  database:\n    model_catalog_endpoint:\n      # -- Username for Model Catalog Apache Jena Fuseki database\n      username: admin\n      # -- Password for Model Catalog Apache Jena Fuseki database\n      password: CHANGEME\n</code></pre>"},{"location":"admin-guide/catalog-configuration/model-catalog-configuration/#initial-data","title":"Initial Data","text":"<p>Helm charts are configured to populate the database with initial data. You can configure the initial data by editing the <code>values.yaml</code> file in the <code>mint</code> directory. The <code>values.yaml</code> file contains the following sections:</p> <p>Note</p> <p>The initial data file must be in the Turtle format. The process of loading the initial data is done only once when the Model Catalog Endpoint is installed.</p> <pre><code>components:\n  model_catalog_endpoint:\n    environment:\n      seeds_url: https://raw.githubusercontent.com/mintproject/model-catalog-endpoint/main/data/wifire-2023-09-22.trig\n</code></pre> <p>The <code>seeds_url</code> key specifies the URL of the initial data. You can change the URL to point to a different initial data file. The initial data file must be in the Turtle format.</p>"},{"location":"admin-guide/catalog-configuration/region-configuration/","title":"Region configuration","text":"<p>Helm charts are configured to populate the database with initial data containing multiple regions. You can remove the regions by querying the database and deleting the regions. The following sections describe how to query the database and delete the regions.</p>"},{"location":"admin-guide/catalog-configuration/region-configuration/#open-the-database","title":"Open the database","text":"<p>To open the database, use the following command:</p> <pre><code>$ kubectl exec -ti $(kubectl get pod -l app=mint-hasura -o jsonpath='{.items[0].metadata.name}') -c hasura-db -- sh\n</code></pre>"},{"location":"admin-guide/catalog-configuration/region-configuration/#query-the-database","title":"Query the database","text":"<p>To query the database, use the following command:</p> <pre><code>$ psql -U postgres -d hasura\n</code></pre> <p>The following SQL query will remove all region geometry that are not California:</p> <pre><code>SELECT\n    rg.region_id\nFROM\n    region\nINNER JOIN\n    region_geometry rg\nON\n    region.id = rg.region_id\nWHERE\n    region.parent_region_id IS NULL\n    AND region.parent_region_id != 'california';\n</code></pre> <pre><code>DELETE FROM region_geometry\nUSING region\nWHERE region_geometry.region_id = region.id\nAND region.parent_region_id != 'california';\n</code></pre> <p>The following SQL query will remove all region geometry that are not California and are not a child of California:</p> <pre><code>DELETE FROM region\nWHERE  region.parent_region_id != 'california' AND region.id != 'california';\n</code></pre> <p>Delete all region that the id is not <code>california</code> and the <code>parent_region_id</code> is <code>NULL</code>:</p> <pre><code>DELETE FROM region\nWHERE  region.parent_region_id IS NULL AND region.id != 'california';\n</code></pre>"},{"location":"admin-guide/catalog-configuration/solution-catalog-configuration/","title":"Solution Catalog","text":"<p>MINT uses hasura to store the task, problem, execution and other metadata.</p> <p>The Hasura database is password protected. The default secret is <code>CHANGEME</code> can change the username and password by editing the <code>values.yaml</code> file in the <code>mint</code> directory. The <code>values.yaml</code> file contains the following sections:</p> <p>The default url is http://graphql.mint.local</p> <pre><code>secrets:\n  hasura:\n    # -- Admin secret for Hasura used to access the console\n    admin_secret: CHANGEME\n</code></pre>"},{"location":"admin-guide/catalog-configuration/solution-catalog-configuration/#initial-data","title":"Initial Data","text":"<p>Helm charts are configured to populate the database with initial data.</p> <p>Note</p> <p>The process of loading the initial data is done only once when the Hasura is installed.</p> <p>To delete existing regions, please go to the Region Configuration page.</p>"},{"location":"admin-guide/execution-configuration/execution-configuration-kubernetes/","title":"Kubernetes","text":"<p>MINT can run models as job on Kubernetes clusters. This guide explains how to set up a Kubernetes cluster for MINT execution.</p> <p>Configure the Kubernetes cluster in the <code>values.yaml</code>.</p> <pre><code>external_services:\n  kubernetes:\n    # -- Enable or disable Kubernetes service to run jobs used by Ensemble Manager\n    enabled: true\n    # -- Kubernetes namespace\n    namespace: 'default'\n    # -- Job CPU limit\n    cpu_limit: '256m'\n    # -- Job memory limit\n    memory_limit: '512Mi'\n    # -- Toggle for node affinity. The job will be scheduled on the same node as the Ensemble Manager\n    node_affinity: true\n</code></pre>"},{"location":"admin-guide/execution-configuration/execution-configuration-tapis/","title":"Tapis","text":"<p>MINT can run models as job on Tapis clusters. The models should be existing Tapis apps. This guide explains how to set up a Tapis cluster for MINT execution.</p> <p>TODO: #76 Add instructions for setting up Tapis cluster for MINT execution.</p>"},{"location":"walkthrough/01-exploring-area/","title":"Explore Areas","text":""},{"location":"walkthrough/01-exploring-area/#exploring-areas","title":"Exploring areas","text":"<p>The explore areas section has three main categories: agriculture, hydrology and administrative areas. </p> <p>Each category presents a group of subregions. </p> <p>Clicking a region on the map will search the DYNAMO catalogs for models configured for that region and datasets with data for that region. </p> <p>Clicking on a model or dataset result will send you to its detailed description page.</p>"},{"location":"walkthrough/02-preparing-models/","title":"02 preparing models","text":""},{"location":"walkthrough/02-preparing-models/#preparing-models","title":"Preparing models","text":"<p>In the prepare models section, you can explore the MINT model catalog using the browse and compare capabilities. You can also add and edit your own models and configure them to work for specific regions and parameters. </p> <p>Let's start exploring the model catalog by clicking on Browse Models</p>"},{"location":"walkthrough/02-preparing-models/#exploring-models","title":"Exploring models","text":"<p>The model browser shows a preview of all models in the catalog and allows you to search for them by indicator, variable, region, and more.</p> <p>The model preview shows the model icon and basic model information. Clicking on More details will display the model's full information. </p> <p>This page shows all the information related to the model. As default, it shows the latest model configuration, but you can change to a specific configuration and setup with the top selectors.</p> <p>After that, we can see basic model metadata and below several tabs with more details. </p> <p>The overview tab shows the model purpose, assumptions, and other non-configuration specific data. </p> <p>And a preview of the selected configuration and setup. Relevant information is presented for each case. </p> <p>The Inputs and Outputs tab shows all the inputs and parameters necessary to run this configuration, and the outputs that this model generates. If a setup is selected, this tab also shows the values pre-selected for that configuration setup.  </p> <p>The Variables tab shows all the variables that each file specification requires for this model configuration. </p> <p>Clicking on the input file name shows a table with all the variables for that file. </p> <p>The Example tab shows an example written for this model configuration. Examples are written in <code>markdown</code> and support the addition of images.  </p> <p>The Technical information tab collects all the information needed to run this model, configuration and setup. This includes the direct command to run this model using DAME. </p>"},{"location":"walkthrough/02-preparing-models/#comparing-models","title":"Comparing models","text":"<p>To compare models, go to the prepare models section and click on compare models. </p> <p>A list of all models, versions, configurations, and setups of the MINT model catalog will be shown on the left. Clicking on their names will add them to the comparison table. </p> <p>You can compare setup specific data too: </p>"},{"location":"walkthrough/02-preparing-models/#adding-models-and-versions","title":"Adding models and versions","text":"<p>To add new models and versions to your catalog, go to the prepare models section and click on add models. </p> <p>The add models page presents a form that can be filled out to add a new model to the catalog. Some information should be provided as text, but more complex resources are defined through specific forms. You will see an edit button to the right of each model catalog resource. </p> <p>Clicking this edit button opens a new dialog. Here you can select resources already in the catalog or edit/create new ones.  When creating a new resource, a specific form with all necessary information will appear. Fill it up to create a new resource. For this example, a new category only needs a name and description. </p> <p>More complex resources have more detailed previews, but follow the same principles for creating, editing, and removing them.  Creating complex resources requires more information. This is the form to add a new grid specification to the model catalog. </p> <p>Once a model has been created, you can edit it by going to the prepare models section and clicking on edit models: </p> <p>The edit models page shows a tree on the left with all models and versions defined in the model catalog. Versions are a way to group configurations that share the same software revision. Models are organized by category and all versions are grouped under their corresponding model.</p> <p>On the right you can see a preview of the model. Clicking on the edit button will make it editable. </p> <p>In edit mode, you will see an interface very similar to the add models page we have seen before. </p>"},{"location":"walkthrough/02-preparing-models/#adding-model-configurations-and-setups","title":"Adding model configurations and setups","text":"<p>To add new model configurations and setups, we need to go back to the prepare models section and click on configure models: </p> <p>The configure models page presents a tree similar to the one we have seen on edit models, but this time two more levels are added: Model configurations (green) below versions and setups (blue) below configurations. </p> <p>A configuration refers to a specific software running with a specific set of parameters and files. You can click on the expand button to better see the information this configuration provides.  For input parameters, a configuration provides their description, order, default values and other important metadata.  For input and output files, a description of all the variables that this file must contain is expected. </p> <p>You can add a new configuration by clicking add new configuration or edit an existing one by clicking edit.  You can add, edit or remove parameters for this configuration.  Clicking on the edit parameters button will display a specific form to add parameters for configurations. </p> <p>The same applies to input and output files:   Once we have finished creating our configuration, we can create a new setup for this configuration.</p> <p>We call the most inner level of a configuration a setup. A setup is a configuration with specific values for parameters and files.</p> <p>You can create or select an existing setup by clicking on the left panel:  This page is similar to the one used to edit configurations, but changes what you can do with parameters and files.</p> <p>In a setup, you can not add nor remove parameters nor files. You can only set specific values for each.   For parameters, you can set a pre-selected value or mark the parameter as adjustable for the user on the modeling step.  For input files, you can set a specific file or collections of files to be pre-selected for the modeling step. </p>"},{"location":"walkthrough/03-exploring-data/","title":"03 exploring data","text":""},{"location":"walkthrough/03-exploring-data/#exploring-data","title":"Exploring data","text":"<p>To explore data, you must go to the Browse Datasets section and click on Browse datasets. </p> <p>This page allows you to search datasets by name or description, for time interval or by drawing a polygon on the map. </p> <p>All matching datasets will be shown on the left panel. Clicking on one will change the contents of the page to reflect the data resources on the map, the time interval, and other relevant information. </p>"},{"location":"walkthrough/04-modeling/","title":"04 modeling","text":""},{"location":"walkthrough/04-modeling/#modeling","title":"Modeling","text":"<p>To use the models, you must go to the User Models section and select a problem statement or create a new one. </p> <p>Each problem statement can have multiple tasks. You can edit or remove existing ones.  Creating a task requires you to specify a response of interest and a time interval. </p> <p>Once a task is created, the system will create a default thread. Threads are a way to group similar model executions on the same task.</p> <p>Each task consists of a series of steps that must be followed to run the models correctly. The first step is to select a model configuration setup you want to run from the list of models that matches your task specification. </p> <p>Once a model configuration setup is selected, all pre-selected datasets will be added and the user will be asked to select them. all datasets that were not specified in that setup.  You can click on the expand view button to go through the modeling steps.</p> <p>If there is no data that matches all the variables specified in the input specification, the system will recommend datasets with a partial match. </p> <p>Once all datasets have been selected, we need to set the parameters for this run. Using multiple parameters will generate multiple runs.  Once all parameters are set, we send the runs.</p> <p>The system will show you a preview of each run and update you on the run status. </p> <p>When all runs are done, the tab Results will activate. Here you can see and download all the files generated during your model execution. </p> <p>Some models can even generate a visualization, but this feature is not common at this moment. </p>"},{"location":"walkthrough/05-reporting/","title":"05 reporting","text":""},{"location":"walkthrough/05-reporting/#reporting","title":"Reporting","text":"<p>The MINT UI can generate simple reports for model runs. For this, go to the Prepare reports section. Here you will see all the threads grouped by problem statement. </p> <p>Clicking on a thread name will show you a page with a resume of that thread's execution. </p>"},{"location":"walkthrough/02-preparing-models/","title":"Preparing Models","text":"<p>In the prepare models section, you can explore the DYNAMO model catalog using the browse and compare capabilities. You can also add and edit your own models and configure them to work for specific regions and parameters.</p>"},{"location":"walkthrough/02-preparing-models/#main-features","title":"Main Features","text":"<ul> <li>Browse Models - Explore the model catalog</li> <li>Compare Models - Compare different models and configurations</li> <li>Add Models - Add new models to the catalog</li> <li>Edit Models - Modify existing models</li> <li>Configure Models - Set up model configurations and setups</li> </ul>"},{"location":"walkthrough/02-preparing-models/add-models/","title":"Add Models","text":"<p>To add new models and versions to your catalog, go to the prepare models section and click on edit models. </p> <p>In this page, click on the add new model button. </p>"},{"location":"walkthrough/02-preparing-models/add-models/#add-models-interface","title":"Add Models Interface","text":"<p>The add models page presents a form that can be filled out to add a new model to the catalog. Some information should be provided as text, but more complex resources are defined through specific forms. You will see an edit button to the right of each model catalog resource. </p>"},{"location":"walkthrough/02-preparing-models/add-models/#resource-management","title":"Resource Management","text":"<p>Clicking this edit button opens a new dialog. Here you can select resources already in the catalog or edit/create new ones. </p> <p>When creating a new resource, a specific form with all necessary information will appear. Fill it up to create a new resource. For this example, a new category only needs a name and description. </p>"},{"location":"walkthrough/02-preparing-models/add-models/#complex-resources","title":"Complex Resources","text":"<p>More complex resources have more detailed previews, but follow the same principles for creating, editing, and removing them. </p> <p>Creating complex resources requires more information. This is the form to add a new grid specification to the model catalog. </p>"},{"location":"walkthrough/02-preparing-models/browse-models/","title":"Browse Models","text":"<p>Let's start exploring the model catalog by clicking on Browse Models</p>"},{"location":"walkthrough/02-preparing-models/browse-models/#model-browser","title":"Model Browser","text":"<p>The model browser shows a preview of all models in the catalog and allows you to search for them by indicator, variable, region, and more.</p> <p>The model preview shows the model icon and basic model information. Clicking on More details will display the model's full information. </p>"},{"location":"walkthrough/02-preparing-models/browse-models/#model-details","title":"Model Details","text":"<p>This page shows all the information related to the model. As default, it shows the latest model configuration, but you can change to a specific configuration and setup with the top selectors.</p> <p>After that, we can see basic model metadata and below several tabs with more details. </p>"},{"location":"walkthrough/02-preparing-models/browse-models/#overview-tab","title":"Overview Tab","text":"<p>The overview tab shows the model purpose, assumptions, and other non-configuration specific data. </p> <p>And a preview of the selected configuration and setup. Relevant information is presented for each case. </p>"},{"location":"walkthrough/02-preparing-models/browse-models/#inputs-and-outputs-tab","title":"Inputs and Outputs Tab","text":"<p>The Inputs and Outputs tab shows all the inputs and parameters necessary to run this configuration, and the outputs that this model generates. If a setup is selected, this tab also shows the values pre-selected for that configuration setup.  </p>"},{"location":"walkthrough/02-preparing-models/browse-models/#variables-tab","title":"Variables Tab","text":"<p>The Variables tab shows all the variables that each file specification requires for this model configuration. </p> <p>Clicking on the input file name shows a table with all the variables for that file. </p>"},{"location":"walkthrough/02-preparing-models/browse-models/#example-tab","title":"Example Tab","text":"<p>The Example tab shows an example written for this model configuration. Examples are written in <code>markdown</code> and support the addition of images.  </p>"},{"location":"walkthrough/02-preparing-models/browse-models/#technical-information-tab","title":"Technical Information Tab","text":"<p>The Technical information tab collects all the information needed to run this model, configuration and setup. This includes the direct command to run this model using DAME. </p>"},{"location":"walkthrough/02-preparing-models/compare-models/","title":"Compare Models","text":"<p>To compare models, go to the Browse models section and click on two or more models to compare. Models are added to the compare queue. </p>"},{"location":"walkthrough/02-preparing-models/compare-models/#model-comparison-interface","title":"Model Comparison Interface","text":"<p>A list of all models, versions, configurations, and setups of the DYNAMO model catalog will be shown on the left. Clicking on their names will add them to the comparison table. </p>"},{"location":"walkthrough/02-preparing-models/compare-models/#setup-comparison","title":"Setup Comparison","text":"<p>You can compare setup specific data too: </p>"},{"location":"walkthrough/02-preparing-models/configure-models/","title":"Configure Models","text":"<p>The configuration system in MINT allows you to define how models should run with specific parameters and file requirements. This section explains how to set up and manage model configurations and their setups.</p>"},{"location":"walkthrough/02-preparing-models/configure-models/#accessing-configuration-interface","title":"Accessing Configuration Interface","text":"<p>To add new model configurations and setups, go to the prepare models section and click on configure models: </p>"},{"location":"walkthrough/02-preparing-models/configure-models/#understanding-the-configuration-hierarchy","title":"Understanding the Configuration Hierarchy","text":"<p>The configuration system follows a hierarchical structure:</p> <ul> <li>Models: Top-level organization of related model versions</li> <li>Versions: Group configurations that share the same software revision</li> <li>Configurations: Define specific software parameters and file requirements</li> <li>Setups: Provide specific values for configurations</li> </ul> <p>The configure models page presents a tree view showing this hierarchy:</p> <ul> <li>Models (default color)</li> <li>Versions (grouped under models)</li> <li>Configurations (green) below versions</li> <li>Setups (blue) below configurations   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure-models/#working-with-configurations","title":"Working with Configurations","text":""},{"location":"walkthrough/02-preparing-models/configure-models/#what-is-a-configuration","title":"What is a Configuration?","text":"<p>A configuration defines:</p> <ul> <li>The specific software version to run</li> <li>Required input parameters</li> <li>Input and output file specifications</li> <li>Variable requirements for each file</li> </ul> <p>You can view detailed configuration information by clicking the expand button: </p>"},{"location":"walkthrough/02-preparing-models/configure-models/#managing-parameters","title":"Managing Parameters","text":""},{"location":"walkthrough/02-preparing-models/configure-models/#input-parameters","title":"Input Parameters","text":"<p>Each configuration defines its required parameters with:</p> <ul> <li>Description and purpose</li> <li>Parameter order</li> <li>Default values</li> <li>Data types</li> <li>Validation rules</li> <li>Other metadata   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure-models/#file-specifications","title":"File Specifications","text":"<p>Configurations specify:</p> <ul> <li>Required input files</li> <li>Expected output files</li> <li>Variable requirements for each file</li> <li>File format specifications   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure-models/#creating-and-editing-configurations","title":"Creating and Editing Configurations","text":"<ol> <li> <p>Add New Configuration</p> </li> <li> <p>Click add new configuration to create a new configuration</p> </li> <li> <p>Or click edit to modify an existing one      </p> </li> <li> <p>Manage Parameters</p> </li> <li> <p>Add, edit, or remove parameters</p> </li> <li>Define parameter metadata</li> <li> <p>Set validation rules      </p> </li> <li> <p>Parameter Form</p> </li> <li> <p>Use the parameter form to define:</p> <ul> <li>Parameter names</li> <li>Data types</li> <li>Default values</li> <li>Validation rules    </li> </ul> </li> <li> <p>File Management</p> </li> <li>Define input file requirements</li> <li>Specify output file formats</li> <li>Set variable requirements       </li> </ol>"},{"location":"walkthrough/02-preparing-models/configure-models/#working-with-setups","title":"Working with Setups","text":""},{"location":"walkthrough/02-preparing-models/configure-models/#what-is-a-setup","title":"What is a Setup?","text":"<p>A setup is the most specific level of configuration, providing:</p> <ul> <li>Concrete values for parameters</li> <li>Specific input files</li> <li>Pre-configured settings for the modeling step</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure-models/#creating-and-managing-setups","title":"Creating and Managing Setups","text":"<ol> <li> <p>Access Setup Interface</p> </li> <li> <p>Select or create a setup from the left panel      </p> </li> <li> <p>Setup Configuration</p> </li> <li>Set specific values for parameters</li> <li>Select input files</li> <li>Configure user-adjustable settings       </li> </ol>"},{"location":"walkthrough/02-preparing-models/configure-models/#parameter-configuration-in-setups","title":"Parameter Configuration in Setups","text":"<p>In setups, you can:</p> <ul> <li>Set pre-selected values for parameters</li> <li>Mark parameters as user-adjustable</li> <li>Define parameter ranges</li> <li>Set default values   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure-models/#file-configuration-in-setups","title":"File Configuration in Setups","text":"<p>For input files, you can:</p> <ul> <li>Select specific files</li> <li>Define file collections</li> <li>Set default selections</li> <li>Configure file validation   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure-models/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Configuration Organization</p> </li> <li> <p>Group related configurations under appropriate versions</p> </li> <li>Use clear, descriptive names</li> <li> <p>Document parameter purposes</p> </li> <li> <p>Parameter Management</p> </li> <li> <p>Set appropriate default values</p> </li> <li>Define clear validation rules</li> <li> <p>Document parameter dependencies</p> </li> <li> <p>Setup Creation</p> </li> <li> <p>Create setups for common use cases</p> </li> <li>Provide meaningful default values</li> <li> <p>Mark frequently adjusted parameters as user-adjustable</p> </li> <li> <p>File Management</p> </li> <li>Validate file formats</li> <li>Document required variables</li> <li>Set appropriate file constraints</li> </ol>"},{"location":"walkthrough/02-preparing-models/edit-models/","title":"Edit Models","text":"<p>Once a model has been created, you can edit it by going to the prepare models section and clicking on edit models: </p>"},{"location":"walkthrough/02-preparing-models/edit-models/#edit-models-interface","title":"Edit Models Interface","text":"<p>The edit models page shows a tree on the left with all models and versions defined in the model catalog. Versions are a way to group configurations that share the same software revision. Models are organized by category and all versions are grouped under their corresponding model.</p> <p>On the right you can see a preview of the model. Clicking on the edit button will make it editable. </p>"},{"location":"walkthrough/02-preparing-models/edit-models/#edit-mode","title":"Edit Mode","text":"<p>In edit mode, you will see an interface very similar to the add models page we have seen before. </p>"},{"location":"walkthrough/02-preparing-models/configure/","title":"Configure Models","text":"<p>The configuration system in DYNAMO allows you to define how models should run with specific parameters and file requirements. This section explains how to set up and manage model configurations and their setups.</p>"},{"location":"walkthrough/02-preparing-models/configure/#configuration-system-overview","title":"Configuration System Overview","text":"<p>The configuration system follows a hierarchical structure:</p> <ul> <li>Models: Top-level organization of related model versions</li> <li>Versions: Group configurations that share the same software revision</li> <li>Configurations: Define specific software parameters and file requirements</li> <li>Setups: Provide specific values for configurations</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/#main-features","title":"Main Features","text":"<ul> <li>Getting Started - Access and basic interface</li> <li>Configurations - Working with model configurations</li> <li>Creating Configurations - How to create new configurations</li> <li>Creating Setups - Creating and managing setups</li> <li>Making Executable - Making configurations executable</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/configurations/","title":"Working with Configurations","text":""},{"location":"walkthrough/02-preparing-models/configure/configurations/#what-is-a-configuration","title":"What is a Configuration?","text":"<p>A configuration defines:</p> <ul> <li>Input Files: Required data files and model components</li> <li>Input Parameters: Configuration values and settings</li> <li>Outputs: Expected results and output specifications</li> <li>ComponentLocation - The location of the model component files (required for model execution)</li> </ul> <p>You can view detailed configuration information by clicking the expand button: </p>"},{"location":"walkthrough/02-preparing-models/configure/configurations/#managing-parameters","title":"Managing Parameters","text":""},{"location":"walkthrough/02-preparing-models/configure/configurations/#input-parameters","title":"Input Parameters","text":"<p>Each configuration defines its required parameters with:</p> <ul> <li>Description and purpose</li> <li>Parameter order</li> <li>Default values</li> <li>Data types</li> <li>Validation rules</li> <li>Other metadata   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/configurations/#file-specifications","title":"File Specifications","text":""},{"location":"walkthrough/02-preparing-models/configure/configurations/#input-files","title":"Input Files","text":"<p>Configurations specify:</p> <ul> <li>Required input files</li> <li>File format requirements</li> <li>Variable requirements for each file</li> <li>File validation rules   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/configurations/#output-files","title":"Output Files","text":"<p>Configurations define:</p> <ul> <li>Expected output files</li> <li>Output format specifications</li> <li>Output variable requirements</li> <li>Output validation rules</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/configurations/#component-location","title":"Component Location","text":"<p>The ComponentLocation field is a critical requirement for model execution:</p> <ul> <li>Specifies the URL or path to the model component files</li> <li>Must be a valid and accessible location</li> <li>Typically points to a ZIP file containing the model code</li> <li>Required for both ModelConfiguration and ModelConfigurationSetup</li> </ul> <p>For DYNAMO versions running on TACC systems, the ComponentLocation should point to a Tapis Application. Tapis Applications are containerized applications that can be executed on TACC clusters. For more information about creating and managing Tapis Applications, refer to the cookbook-docker-template documentation.</p>"},{"location":"walkthrough/02-preparing-models/configure/configurations/#next-steps","title":"Next Steps","text":"<p>Now that you understand what configurations are, you can learn about creating and editing configurations. After that, you can proceed to learn about creating and managing setups. Setups allow you to provide specific values and pre-configured settings for your model runs based on these configurations.</p>"},{"location":"walkthrough/02-preparing-models/configure/creating-configurations/","title":"Creating and Editing Configurations","text":""},{"location":"walkthrough/02-preparing-models/configure/creating-configurations/#add-new-configuration","title":"Add New Configuration","text":"<ul> <li>Click add new configuration to create a new configuration</li> <li>Or click edit to modify an existing one   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-configurations/#manage-parameters","title":"Manage Parameters","text":"<ul> <li>Add, edit, or remove parameters</li> <li>Define parameter metadata</li> <li>Set validation rules   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-configurations/#parameter-form","title":"Parameter Form","text":"<ul> <li>Use the parameter form to define:</li> <li>Parameter names</li> <li>Data types</li> <li>Default values</li> <li>Validation rules     </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-configurations/#file-management","title":"File Management","text":""},{"location":"walkthrough/02-preparing-models/configure/creating-configurations/#input-files","title":"Input Files","text":"<ul> <li>Define input file requirements</li> <li>Set file format specifications</li> <li>Configure file validation</li> <li>Specify variable requirements   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-configurations/#output-files","title":"Output Files","text":"<ul> <li>Define output file formats</li> <li>Set output specifications</li> <li>Configure output validation</li> <li>Specify output requirements   </li> </ul> <p>Note</p> <p>To make your configuration executable, you need to set up the ComponentLocation and ensure all input files have variables. See Making your Configuration or ConfigurationSetup Executable for detailed instructions.</p>"},{"location":"walkthrough/02-preparing-models/configure/creating-configurations/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to create and edit configurations, you can proceed to learn about creating and managing setups. Setups allow you to provide specific values and pre-configured settings for your model runs based on these configurations.</p>"},{"location":"walkthrough/02-preparing-models/configure/creating-setups/","title":"Working with Setups","text":"<p>Warning</p> <p>Please make sure you have completed the User guide before you start.</p> <p>A ModelConfigurationSetup is executable if:</p> <ul> <li>Has a valid ComponentLocation</li> <li>All input files have at least one Variable associated or are fixed files</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#what-is-a-setup","title":"What is a Setup?","text":"<p>A setup is the most specific level of configuration, providing concrete values for:</p> <ul> <li>Input Files: Required data files and model components</li> <li>Input Parameters: Configuration values and settings</li> <li>Outputs: Expected results and output specifications</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#creating-and-managing-setups","title":"Creating and Managing Setups","text":""},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#1-access-setup-interface","title":"1. Access Setup Interface","text":"<ul> <li>Select or create a setup from the left panel   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#2-setup-configuration","title":"2. Setup Configuration","text":""},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#input-files","title":"Input Files","text":"<ul> <li>Select specific input data files</li> <li>Configure model component locations</li> <li>Set file validation rules</li> <li>Define file collections   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#input-parameters","title":"Input Parameters","text":"<ul> <li>Set specific parameter values</li> <li>Configure user-adjustable settings</li> <li>Define parameter ranges</li> <li>Set default values   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#outputs","title":"Outputs","text":"<ul> <li>Define expected output files</li> <li>Specify output formats</li> <li>Configure output validation</li> <li>Set output naming conventions   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#parameter-configuration-in-setups","title":"Parameter Configuration in Setups","text":"<p>In setups, you can:</p> <ul> <li>Set pre-selected values for parameters</li> <li>Mark parameters as user-adjustable</li> <li>Define parameter ranges</li> <li>Set default values   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#file-configuration-in-setups","title":"File Configuration in Setups","text":"<p>For input files, you can:</p> <ul> <li>Select specific files</li> <li>Define file collections</li> <li>Set default selections</li> <li>Configure file validation   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#output-configuration-in-setups","title":"Output Configuration in Setups","text":"<p>For outputs, you can:</p> <ul> <li>Define output file formats</li> <li>Specify output variable requirements</li> <li>Set output validation rules</li> <li>Configure output naming patterns</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/creating-setups/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to create and manage setups, proceed to making your configuration executable to ensure your setup can be run successfully.</p>"},{"location":"walkthrough/02-preparing-models/configure/getting-started/","title":"Getting Started with Configurations","text":""},{"location":"walkthrough/02-preparing-models/configure/getting-started/#accessing-the-configuration-interface","title":"Accessing the Configuration Interface","text":"<p>To add new model configurations and setups, go to the prepare models section and click on configure models: </p>"},{"location":"walkthrough/02-preparing-models/configure/getting-started/#configuration-interface","title":"Configuration Interface","text":"<p>The configure models page presents a tree view showing the configuration hierarchy:</p> <ul> <li>Models (default color)</li> <li>Versions (grouped under models)</li> <li>Configurations (green) below versions</li> <li>Setups (blue) below configurations   </li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/getting-started/#understanding-the-tree-view","title":"Understanding the Tree View","text":"<p>The tree view provides a visual representation of your model's configuration structure:</p>"},{"location":"walkthrough/02-preparing-models/configure/getting-started/#1-models-level","title":"1. Models Level","text":"<ul> <li>Top-level organization</li> <li>Groups related model versions</li> <li>Provides model-wide settings</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/getting-started/#2-versions-level","title":"2. Versions Level","text":"<ul> <li>Groups configurations by software revision</li> <li>Maintains version-specific settings</li> <li>Links to software documentation</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/getting-started/#3-configurations-level","title":"3. Configurations Level","text":"<ul> <li>Defines specific parameter sets</li> <li>Specifies file requirements</li> <li>Sets up validation rules</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/getting-started/#4-setups-level","title":"4. Setups Level","text":"<ul> <li>Provides concrete values</li> <li>Pre-configures model runs</li> <li>Defines user-adjustable parameters</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/getting-started/#basic-navigation","title":"Basic Navigation","text":"<ul> <li>Click on any item in the tree to view its details</li> <li>Use the expand/collapse buttons to manage the view</li> <li>Right-click for additional options</li> <li>Use the search function to find specific items</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basic structure of the configuration interface, you can proceed to learn about working with configurations in detail. This will show you how to define parameters, specify file requirements, and create effective model configurations.</p>"},{"location":"walkthrough/02-preparing-models/configure/making-executable/","title":"Making your Configuration or ConfigurationSetup Executable","text":"<p>Warning</p> <p>A ModelConfiguration or ModelConfigurationSetup is executable if:</p> <ul> <li>Has a valid ComponentLocation</li> <li>All input files have at least one Variable associated or are fixed files</li> </ul>"},{"location":"walkthrough/02-preparing-models/configure/making-executable/#setting-component-location","title":"Setting Component Location","text":"<p>The ComponentLocation is a critical field that specifies where the model code is located:</p>"},{"location":"walkthrough/02-preparing-models/configure/making-executable/#classic-mint","title":"Classic MINT","text":"<ol> <li>In the configuration form, locate the \"Component Location\" field</li> <li>Enter the URL to your model component files:</li> <li>For standard MINT deployments: Use a GitHub URL pointing to a ZIP file</li> <li>For TACC deployments: Use a Tapis Application URL</li> </ol>"},{"location":"walkthrough/02-preparing-models/configure/making-executable/#tacc-mint","title":"TACC MINT","text":"<ol> <li>In the configuration form, locate the \"Component Location\" field</li> <li>Click the expand button to view the Component Location field</li> <li>Select the Tapis Application from the options provided</li> </ol> <p>Note</p> <p>For TACC deployments, make sure your ComponentLocation points to a valid Tapis Application. Refer to the cookbook-docker-template documentation for creating Tapis Applications.</p>"},{"location":"walkthrough/02-preparing-models/configure/making-executable/#adding-variables-to-input-files","title":"Adding Variables to Input Files","text":"<p>To make your configuration executable, each input file must have at least one variable associated or be a fixed file. Follow these steps to add a variable to an input file:</p> <ol> <li>Click the edit button on the input file.    </li> <li>In the \"Variables\" section, search for and select the variable(s) you want to associate with the input file.    </li> <li>Save your changes.</li> </ol> <p>This ensures that the input file is properly specified and can be used for model execution.</p>"},{"location":"walkthrough/walkthrough/","title":"Prepare Data","text":"<p>This guide explains how to add data to MINT. There are two main options for adding datasets:</p> <ol> <li>MINT Data Catalog - Add data directly through the MINT platform's integrated interface</li> <li>CKAN - TACC Deployment - Use the open-source data management system to host and share datasets</li> </ol>"},{"location":"walkthrough/walkthrough/#key-requirements","title":"Key Requirements","text":"<p>Regardless of which method you choose, there are some important requirements to keep in mind:</p>"},{"location":"walkthrough/walkthrough/#spatial-coverage","title":"Spatial Coverage","text":"<p>Each dataset must include a spatial coverage field as a GeoJSON object describing the geographic area covered by the dataset.</p>"},{"location":"walkthrough/walkthrough/#mint-standard-variables","title":"MINT Standard Variables","text":"<p>Each resource must specify at least one MINT Standard Variable to ensure interoperability and consistency across datasets.</p> <p>For more information about standard variables, refer to the list of standard variables and the MINT documentation.</p>"},{"location":"walkthrough/walkthrough/03-1-preparing-data-ckan/","title":"Prepare Data: CKAN","text":"<p>This guide explains how to add data to DYNAMO using CKAN.</p>"},{"location":"walkthrough/walkthrough/03-1-preparing-data-ckan/#adding-data-via-ckan","title":"Adding Data via CKAN","text":"<p>CKAN is an open-source data management system that can be used to host and share datasets. When adding data to DYNAMO via CKAN, there are two important requirements:</p>"},{"location":"walkthrough/walkthrough/03-1-preparing-data-ckan/#1-spatial-coverage","title":"1. Spatial Coverage","text":"<p>Each dataset must include a spatial coverage field. This is typically a GeoJSON object describing the geographic area covered by the dataset. For example:</p> <pre><code>{\n  \"type\": \"Polygon\",\n  \"coordinates\": [\n    [\n      [-98.0, 30.3],\n      [-97.5255, 30.3],\n      [-97.5255, 30.155],\n      [-98.0, 30.155],\n      [-98.0, 30.3]\n    ]\n  ]\n}\n</code></pre> <p>This field ensures that the dataset can be properly indexed and discovered based on its geographic relevance.</p>"},{"location":"walkthrough/walkthrough/03-1-preparing-data-ckan/#2-dynamo-standard-variables","title":"2. DYNAMO Standard Variables","text":"<p>Each resource in the dataset must specify at least one DYNAMO Standard Variable. These are standardized variable names used within the DYNAMO platform to ensure interoperability and consistency across datasets. For example:</p> <pre><code>groundwater__initial_head\n</code></pre> <p>You can select from the list of standard variables provided in the DYNAMO interface. This step is required for the dataset to be usable in DYNAMO workflows.</p> <p></p>"},{"location":"walkthrough/walkthrough/03-1-preparing-data-ckan/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Set up your CKAN instance</li> <li>Create a new dataset</li> <li>Add the required spatial coverage field</li> <li>Add resources and specify DYNAMO Standard Variables</li> <li>Publish the dataset</li> </ol> <p>For more information, refer to the list of standard variables and the DYNAMO documentation.</p>"},{"location":"walkthrough/walkthrough/03-1-preparing-data-dynamo-catalog/","title":"Prepare Data: MINT Data Catalog","text":"<p>Documentation for the MINT Data Catalog is coming soon.</p>"}]}